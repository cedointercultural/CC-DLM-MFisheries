{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando activaciones de model\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Procesando activaciones de model_1\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "Procesando activaciones de model_2\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Cargar el modelo\n",
    "model_path = 'modelos_moe/CORVINA_cluster_7_moe_model.h5'\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Crear una entrada de prueba con la misma forma esperada por la capa de entrada del modelo\n",
    "test_input = np.random.random((1, 6, 14))  # Ajusta las dimensiones según la estructura del modelo\n",
    "\n",
    "\n",
    "# Obtener las activaciones de los submodelos\n",
    "def get_submodel_activations(submodel, input_data):\n",
    "    layer_outputs = [layer.output for layer in submodel.layers]\n",
    "    activation_model = tf.keras.models.Model(inputs=submodel.input, outputs=layer_outputs)\n",
    "    activations = activation_model.predict(input_data)\n",
    "    return activations\n",
    "\n",
    "# Recorremos los submodelos individualmente\n",
    "submodel_activations = {}\n",
    "\n",
    "# Verifica los nombres de los submodelos en tu modelo\n",
    "for layer in model.layers:\n",
    "    if 'model' in layer.name:  # Esto verifica si es un submodelo\n",
    "        print(f\"Procesando activaciones de {layer.name}\")\n",
    "        activations = get_submodel_activations(layer, test_input)\n",
    "        submodel_activations[layer.name] = activations\n",
    "\n",
    "# Ahora ya tenemos las activaciones de cada submodelo\n",
    "# Puedes iterar sobre submodel_activations y graficar cada submodelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Filtrar los datos por especie y clúster\n",
    "species_to_filter = \"MOJARRA\"  # Especie de interés\n",
    "cluster_to_filter = 7            # Clúster de interés\n",
    "\n",
    "# Cargar los datos históricos y futuros\n",
    "data = pd.read_csv('data.csv')\n",
    "future_data = pd.read_csv('future_data.csv')\n",
    "\n",
    "# Filtrar los datos según la especie y el clúster\n",
    "filtered_data = data[(data['species'] == species_to_filter) & (data['Cluster_Label'] == cluster_to_filter)]\n",
    "\n",
    "# Cargar el modelo y el scaler\n",
    "model_path = 'modelos_moe/CORVINA_cluster_7_moe_model.h5'  # Cambiar al path real si es diferente\n",
    "scaler_path = 'scaler/scaler.pkl'  # Cambiar al path real si es diferente\n",
    "model = load_model(model_path)\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "# Seleccionar las características de entrada\n",
    "features = ['landed_w_kg', 'Cluster_Label', 'mean_temp_30m', 'mean_temp_10m', 'thetao_sfc=6',\n",
    "            'thetao_sfc=7.92956018447876', 'thetao_sfc=9.572997093200684', 'thetao_sfc=11.40499973297119',\n",
    "            'thetao_sfc=13.46714019775391', 'thetao_sfc=15.8100700378418', 'thetao_sfc=18.49555969238281',\n",
    "            'thetao_sfc=21.59881973266602', 'thetao_sfc=25.21141052246094', 'thetao_sfc=29.44473075866699']\n",
    "\n",
    "# Escalar los datos de entrenamiento\n",
    "group_scaled = scaler.transform(filtered_data[features])\n",
    "\n",
    "# Preparar los datos para el pronóstico\n",
    "look_back = 6\n",
    "X_input = group_scaled[-look_back:].astype(np.float32)\n",
    "X_input = np.reshape(X_input, (1, X_input.shape[0], X_input.shape[1]))\n",
    "\n",
    "# Realizar las predicciones utilizando los datos futuros\n",
    "predictions = []\n",
    "for i in range(len(future_data)):\n",
    "    # Realizar la predicción\n",
    "    pred = model.predict(X_input)\n",
    "    predictions.append(pred[0][0])\n",
    "    \n",
    "    # Actualizar la entrada para la siguiente predicción\n",
    "    future_temp_values = future_data.iloc[i][['mean_temp_30m', 'mean_temp_10m', 'thetao_sfc=6', \n",
    "                                              'thetao_sfc=7.92956018447876', 'thetao_sfc=9.572997093200684',\n",
    "                                              'thetao_sfc=11.40499973297119', 'thetao_sfc=13.46714019775391',\n",
    "                                              'thetao_sfc=15.8100700378418', 'thetao_sfc=18.49555969238281',\n",
    "                                              'thetao_sfc=21.59881973266602', 'thetao_sfc=25.21141052246094',\n",
    "                                              'thetao_sfc=29.44473075866699']].values.reshape(1, -1).astype(np.float32)\n",
    "    new_record = np.hstack((pred, np.array([[filtered_data['Cluster_Label'].iloc[0]]], dtype=np.float32), future_temp_values))\n",
    "    new_record_scaled = scaler.transform(new_record)  # Escalar el nuevo registro\n",
    "    new_record_scaled = new_record_scaled.reshape((1, 1, -1))\n",
    "    X_input = np.append(X_input[:, 1:, :], new_record_scaled, axis=1)\n",
    "\n",
    "predictions = np.array(predictions).reshape(-1, 1)\n",
    "predictions = scaler.inverse_transform(np.hstack((predictions, np.zeros((len(predictions), group_scaled.shape[1] - 1)))))[:, 0]\n",
    "\n",
    "# Análisis SHAP por década de pronóstico\n",
    "decade_intervals = len(predictions) // 10  # Dividir las predicciones en intervalos de décadas\n",
    "\n",
    "# Crear una función envolvente para que el modelo pueda recibir entradas de 2D y transformarlas en 3D\n",
    "def predict_wrapper(input_data):\n",
    "    input_data_reshaped = input_data.reshape(-1, look_back, len(features))\n",
    "    return model.predict(input_data_reshaped).flatten()\n",
    "\n",
    "# Inicializar KernelExplainer y realizar el análisis SHAP para cada década\n",
    "for decade in range(0, len(predictions), decade_intervals):\n",
    "    # Preparar los datos de entrada para el análisis SHAP\n",
    "    X_decade = group_scaled[-look_back:].astype(np.float32)\n",
    "    X_decade = np.reshape(X_decade, (1, look_back, len(features)))\n",
    "    \n",
    "    # Inicializar KernelExplainer con la muestra del conjunto de datos\n",
    "    background = X_decade.reshape(-1, look_back * len(features))  # Aplanar la muestra de fondo para KernelExplainer\n",
    "    explainer = shap.KernelExplainer(predict_wrapper, background)\n",
    "    \n",
    "    # Calcular los valores SHAP para la década actual\n",
    "    shap_values = explainer.shap_values(background, nsamples=50)\n",
    "    \n",
    "    # Crear una lista de nombres para todas las características en la entrada aplanada\n",
    "    flattened_feature_names = []\n",
    "    for timestep in range(look_back):\n",
    "        for feature in features:\n",
    "            flattened_feature_names.append(f\"{feature}_timestep_{timestep + 1}\")\n",
    "    \n",
    "    # Mostrar la gráfica SHAP con los nombres correctos para la década\n",
    "    print(f\"Análisis SHAP para la década {decade // decade_intervals + 1}\")\n",
    "    shap.summary_plot(shap_values, feature_names=flattened_feature_names, plot_type=\"bar\")\n",
    "\n",
    "# Mostrar las predicciones futuras\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(predictions, label='Predicted Landed Weight (kg)', marker='o')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Landed Weight (kg)')\n",
    "plt.title(f'Predictions for {species_to_filter} in Cluster {cluster_to_filter}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pro24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
